{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import random\n","import wandb\n","wandb.login(key=\"279a68e0fd5d16d5893ca46bdc25076ad1f3be50\")\n","wandb.init(project=\"DLAssignment3\",entity=\"cs22m072\")\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m072\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.15.3 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_072805-enwgjy6k</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cs22m072/DLAssignment3/runs/enwgjy6k' target=\"_blank\">hopeful-aardvark-102</a></strong> to <a href='https://wandb.ai/cs22m072/DLAssignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cs22m072/DLAssignment3' target=\"_blank\">https://wandb.ai/cs22m072/DLAssignment3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cs22m072/DLAssignment3/runs/enwgjy6k' target=\"_blank\">https://wandb.ai/cs22m072/DLAssignment3/runs/enwgjy6k</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import random\n","import os\n","import wandb\n","wandb.login(key=\"279a68e0fd5d16d5893ca46bdc25076ad1f3be50\")\n","wandb.init(project=\"DLAssignment3\",entity=\"cs22m072\")\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, csv_file):\n","        self.data = pd.read_csv(csv_file,names=[\"English\",\"Hindi\"],header=None)\n","        \n","    def __getitem__(self, index):\n","        x = self.data.iloc[index][\"English\"]\n","        y = self.data.iloc[index][\"Hindi\"]\n","        return x, y\n","    \n","    def __len__(self):\n","        return len(self.data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = MyDataset(os.getcwd()+'/hin_train.csv')\n","test_data = MyDataset(os.getcwd()+'/hin_test.csv')\n","val_data = MyDataset(os.getcwd()+'/hin_valid.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","ENGLEN=32\n","HINDILEN=32\n","BATCH_SIZE=128\n","englishwords=torch.full((len(train_data), ENGLEN), 2).to(device)\n","hindiwords=torch.full((len(train_data), HINDILEN), 2).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['0', '1', '2', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"]}],"source":["hindivocab=set()\n","englishvocab=set()\n","for x,y in train_data:\n","    for letter in x:\n","        englishvocab.add(letter)\n","    for letter in y:\n","        hindivocab.add(letter)  \n","for x,y in test_data:\n","    for letter in x:\n","        englishvocab.add(letter)\n","    for letter in y:\n","        hindivocab.add(letter)\n","for x,y in test_data:\n","    for letter in x:\n","        englishvocab.add(letter)\n","    for letter in y:\n","        hindivocab.add(letter)\n","hindivocab=list(hindivocab)\n","hindivocab.sort()\n","englishvocab=list(englishvocab)\n","englishvocab.sort()\n","hindivocab.insert(0,'0')#start\n","hindivocab.insert(1,'1') #end\n","hindivocab.insert(2,'2') #pad\n","englishvocab.insert(0,'0')#start\n","englishvocab.insert(1,'1') #end\n","englishvocab.insert(2,'2') #pad\n","print(englishvocab)\n","hindidictc={}\n","englishdictc={}\n","hindidicti={}\n","englishdicti={}\n","for i in range(len(hindivocab)):\n","    hindidicti[i]=hindivocab[i]\n","    hindidictc[hindivocab[i]]=i\n","for i in range(len(englishvocab)):\n","    englishdicti[i]=englishvocab[i]\n","    englishdictc[englishvocab[i]]=i\n","\n","c=0\n","for x,y in train_data:\n","    for i in range(len(x)):\n","        englishwords[c][i]=englishdictc[x[i]]\n","    for i in range(len(y)):\n","        hindiwords[c][i]=hindidictc[y[i]]\n","    hindiwords[c][i+1]=1\n","    c+=1\n","\n","englishwordsval=torch.full((len(val_data), ENGLEN), 2).to(device)\n","hindiwordsval=torch.full((len(val_data), HINDILEN), 2).to(device)\n","c=0\n","for x,y in val_data:\n","    for i in range(len(x)):\n","        englishwordsval[c][i]=englishdictc[x[i]]\n","    for i in range(len(y)):\n","        hindiwordsval[c][i]=hindidictc[y[i]]\n","    hindiwordsval[c][i+1]=1\n","    c+=1\n","    \n","englishwordstest=torch.full((len(test_data), ENGLEN), 2).to(device)\n","hindiwordstest=torch.full((len(test_data), HINDILEN), 2).to(device)\n","c=0\n","for x,y in test_data:\n","    for i in range(len(x)):\n","        englishwordstest[c][i]=englishdictc[x[i]]\n","    for i in range(len(y)):\n","        hindiwordstest[c][i]=hindidictc[y[i]]\n","    hindiwordstest[c][i+1]=1\n","    c+=1"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Encoder Decoder Seq2seq"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size,hidden_size,embedding_size,num_layers,typecell,dropout,bidirectional):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(dropout)\n","        self.num_layers=num_layers\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.bidirectional=bidirectional\n","        self.p=dropout\n","        self.typecell=typecell\n","        if self.typecell==\"gru\":\n","            self.step = nn.GRU(embedding_size, hidden_size,num_layers,dropout=self.p,bidirectional=self.bidirectional)    \n","        if self.typecell==\"lstm\":\n","            self.step = nn.LSTM(embedding_size, hidden_size,num_layers,dropout=self.p,bidirectional=self.bidirectional)\n","        if self.typecell==\"rnn\":\n","            self.step = nn.RNN(embedding_size, hidden_size,num_layers,dropout=self.p,bidirectional=self.bidirectional) \n","\n","    def forward(self, inp, hidden,cell=None):\n","        embedded = self.dropout(self.embedding(inp))\n","        if self.typecell==\"gru\":\n","            output, hidden = self.step(embedded, hidden)   \n","            return output,hidden\n","        if self.typecell==\"rnn\":\n","            output, hidden = self.step(embedded, hidden)   \n","            return output,hidden\n","        if self.typecell==\"lstm\": \n","            output, (hidden,cell) = self.step(embedded, (hidden,cell))\n","            return output, (hidden,cell)\n","\n","    def initHidden(self):\n","        #for bidirection\n","        num_layers=self.num_layers\n","        if self.bidirectional:\n","            num_layers=self.num_layers*2\n","        hidden=torch.zeros(num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","        if self.typecell==\"lstm\":\n","            cell=torch.zeros(num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","            return (hidden,cell)        \n","        return hidden\n","\n","\n","    \n","class DecoderRNN(nn.Module):\n","    def __init__(self,input_size,hidden_size,embedding_size,num_layers,output_size,typecell,dropout):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(dropout)\n","        self.num_layers=num_layers\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.output_size=output_size\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.Softmax(dim=2)\n","        self.p=dropout\n","        self.typecell=typecell\n","        if self.typecell==\"gru\":\n","            self.step = nn.GRU(embedding_size, hidden_size,num_layers,dropout=self.p)   \n","        if self.typecell==\"rnn\":\n","            self.step = nn.RNN(embedding_size, hidden_size,num_layers,dropout=self.p) \n","        if self.typecell==\"lstm\":\n","            self.step = nn.LSTM(embedding_size, hidden_size,num_layers,dropout=self.p)\n","\n","    def forward(self, inp, hidden,cell=None):\n","        embedded = self.dropout(self.embedding(inp))\n","        if self.typecell==\"gru\":\n","            output, hidden = self.step(embedded, hidden)   \n","            output1=self.out(output)\n","            return output1,hidden\n","        if self.typecell==\"rnn\":\n","            output, hidden = self.step(embedded, hidden)   \n","            output1=self.out(output)\n","            return output1,hidden\n","        if self.typecell==\"lstm\": \n","            output, (hidden,cell) = self.step(embedded,  (hidden,cell))\n","            output1=self.out(output)\n","            return output1, (hidden,cell)\n","\n","    def initHidden(self):\n","        #for bidirection\n","        num_layers=self.num_layers\n","        hidden=torch.zeros(num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","        if self.typecell==\"lstm\":\n","            cell=torch.zeros(num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","            return (hidden,cell)        \n","        return hidden \n","            \n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder,hencoder,cell=None):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.hencoder=hencoder\n","        self.cell=cell\n","    def forward(self, inp, target,teacher_force_ratio):\n","        outputs = torch.zeros(HINDILEN,BATCH_SIZE ,len(hindivocab)).to(device)\n","        if self.encoder.typecell==\"lstm\":\n","            p,(hencoder3d,cell)=self.encoder.forward(inp.to(device),self.hencoder,self.cell)\n","        else:\n","            p,hencoder3d=self.encoder.forward(inp.to(device),self.hencoder)\n","        tempdecoder=torch.zeros(self.encoder.num_layers,BATCH_SIZE,hencoder3d.size()[2]).to(device)\n","        tempdecoder[0]=hencoder3d[hencoder3d.size()[0]//2-1]\n","        tempdecoder[1]=hencoder3d[(hencoder3d.size()[0]//2)*2-1]\n","        hdecoder=torch.add(tempdecoder[0],tempdecoder[1])\n","        hdecoder=hdecoder.repeat(self.decoder.num_layers,1,1)\n","        if self.encoder.typecell==\"lstm\":\n","            tempcell=torch.zeros(2,BATCH_SIZE,cell.size()[2]).to(device)\n","            tempcell[0]=hencoder3d[cell.size()[0]//2-1]\n","            tempcell[1]=hencoder3d[(cell.size()[0]//2)*2-1]\n","            cell=torch.add(tempcell[0],tempcell[1])\n","            cell=cell.repeat(self.decoder.num_layers,1,1)\n","        x=torch.full((1,BATCH_SIZE),hindidictc['0'])\n","        if self.encoder.typecell==\"lstm\":\n","            output,(hdecoder,cell)=self.decoder.forward(x.to(device),hdecoder,cell)\n","        else:\n","            output,hdecoder=self.decoder.forward(x.to(device),hdecoder)\n","        outputs[0]=output\n","        t=1\n","        for i in range(1,HINDILEN):\n","            if random.random() > teacher_force_ratio:\n","                output=self.decoder.softmax(output)\n","                nextinp=torch.argmax(output, dim=2)\n","                if self.encoder.typecell==\"lstm\":\n","                    output,(hdecoder,cell)=self.decoder.forward(nextinp.to(device),hdecoder,cell)\n","                else:\n","                    output,hdecoder=self.decoder.forward(nextinp.to(device),hdecoder)\n","                outputs[t]=output\n","                t+=1\n","            else:\n","                nextinp=target[i-1,:].unsqueeze(0)\n","                if self.encoder.typecell==\"lstm\":\n","                    output,(hdecoder,cell)=self.decoder.forward(nextinp.to(device),hdecoder,cell)\n","                else:\n","                    output,hdecoder=self.decoder.forward(nextinp.to(device),hdecoder)\n","                outputs[t]=output\n","                t+=1\n","        return outputs\n","        \n","    \n","alpha=[]\n","def train(encoder,decoder,seq2seq,epoch):\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","    loss=0\n","    count=0\n","    numbatches=englishwords.shape[0]//BATCH_SIZE\n","    for ep in range(epoch):\n","        trainloss=0\n","        train_correct=0\n","        for i in range(numbatches):\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            temp=englishwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temph=hindiwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temp=temp.t()\n","            temph=temph.t()\n","            output=seq2seq.forward(temp,temph,0.5)\n","            train_correct+=train_accuracy(output,temph)\n","            output = output[:].reshape(-1, output.shape[2])\n","            tem = temph[:].reshape(-1)\n","            loss=criterion(output,tem)\n","            loss.backward()\n","            trainloss+=loss.item()\n","            torch.nn.utils.clip_grad_norm_(decoder.parameters(),max_norm = 1)\n","            torch.nn.utils.clip_grad_norm_(encoder.parameters(),max_norm = 1)\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","        val_correct,cur_loss=accuracy(seq2seq,englishwordsval,hindiwordsval)\n","        print(ep,trainloss/(51200*HINDILEN),cur_loss/(4096*HINDILEN),val_correct,train_correct)\n","        trainloss=trainloss/(51200*HINDILEN)\n","        cur_loss=cur_loss/(4096*HINDILEN)\n","        tra_acc=train_correct/51200\n","        val_acc=val_correct/4096\n","        wandb.log({\"Training loss\":trainloss,'Val loss':cur_loss,'Training Accuracy':tra_acc,'Val Accuracy':val_acc})\n","\n","def train_accuracy(output,temph):\n","        output=nn.Softmax(dim=2)(output)\n","        output=torch.argmax(output,dim=2)\n","        temph=temph.t()\n","        output=output.t()\n","        correct=0\n","        for i in range(BATCH_SIZE):\n","            if(torch.equal(output[i],temph[i])):\n","                correct+=1\n","        return correct\n","        \n","def accuracy(seq2seq,english,hindi):\n","    numbatches=english.shape[0]//BATCH_SIZE\n","    correct=0\n","    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","    loss=0\n","    for i in range(numbatches):\n","        temp=english[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temph=hindi[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temp=temp.t()\n","        temph=temph.t()\n","        output=seq2seq.forward(temp,temph,0)\n","        o = output[:].reshape(-1, output.shape[2])\n","        tem = temph[:].reshape(-1)\n","        x=criterion(o,tem)\n","        loss+=x.item()\n","        output=nn.Softmax(dim=2)(output)\n","        output=torch.argmax(output,dim=2)\n","        temph=temph.t()\n","        output=output.t()\n","        for i in range(BATCH_SIZE):\n","            if(torch.equal(output[i],temph[i])):\n","                correct+=1\n","\n","    return correct,loss\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Sweep code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sweep():\n","    wandb.init(project='DLAssignment3')\n","    config = wandb.config\n","    wandb.run.name = \"cellType_{}_embSize_{}_layers_{}_batchSize_{}_hidden_{}_dropout_{}\".format(config.cell_type,config.input_embedding_size,config.no_of_layers,config.batchsize,config.hidden_size,config.dropout)\n","    hidden_size = config.hidden_size\n","    char_embed_size = config.input_embedding_size\n","    no_of_layers = config.no_of_layers\n","    epochs = config.epochs\n","    batchsize = config.batchsize\n","    dropout = config.dropout\n","    cell_type=config.cell_type\n","    if(config.bidirectional=='Yes'):\n","        bidirectional=True\n","    else:\n","        bidirectional=False\n","    encoder=EncoderRNN(len(englishvocab),hidden_size,char_embed_size,no_of_layers,cell_type,dropout,bidirectional).to(device)\n","    decoder=DecoderRNN(len(hindivocab),hidden_size,char_embed_size,no_of_layers,len(hindivocab),cell_type,dropout).to(device)\n","    if cell_type=='lstm':\n","        hencoder,cell=encoder.initHidden()\n","        seq2seq=Seq2Seq(encoder,decoder,hencoder,cell)\n","    else:\n","        hencoder=encoder.initHidden()\n","        seq2seq=Seq2Seq(encoder,decoder,hencoder)\n","    train(encoder,decoder,seq2seq,epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_configuration = {\n","    'method' : 'bayes',\n","    'metric' : { 'goal' : 'maximize',\n","    'name' : 'Val_Accuracy'},\n","    'parameters':{\n","        'batchsize' : {'values' : [64,128,256]},\n","        'input_embedding_size' : {'values' : [128,256,512]},\n","        'no_of_layers' : {'values' : [2,3,4,5]},\n","        'hidden_size' : {'values' : [128,256,512]},\n","        'cell_type' : {'values' : ['lstm','rnn','gru']},\n","        'bidirectional' : {'values' : ['Yes']},\n","        'dropout' : {'values' : [0.2,0.4]},\n","        'epochs' : {'values' : [10,15,20]}\n","    }\n","}\n","sweep_id = wandb.sweep(sweep = sweep_configuration,project = 'DLAssignment3')\n","wandb.agent(sweep_id,function=sweep)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Encoder Decoder Seq2seq for Attention"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size,hidden_size,embedding_size,num_layers,typecell,dropout,bidirectional):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(dropout)\n","        self.num_layers=num_layers\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.bidirectional=bidirectional\n","        self.p=dropout\n","        self.typecell=typecell\n","        if self.typecell==\"gru\":\n","            self.step = nn.GRU(embedding_size, hidden_size,num_layers,dropout=self.p,bidirectional=self.bidirectional)    \n","        if self.typecell==\"lstm\":\n","            self.step = nn.LSTM(embedding_size, hidden_size,num_layers,dropout=self.p,bidirectional=self.bidirectional)\n","        if self.typecell==\"rnn\":\n","            self.step = nn.RNN(embedding_size, hidden_size,num_layers,dropout=self.p,bidirectional=self.bidirectional) \n","\n","    def forward(self, inp, hidden,cell=None):\n","        embedded = self.dropout(self.embedding(inp))\n","        if self.typecell==\"gru\":\n","            output, hidden = self.step(embedded, hidden)   \n","            return output,hidden\n","        if self.typecell==\"rnn\":\n","            output, hidden = self.step(embedded, hidden)   \n","            return output,hidden\n","        if self.typecell==\"lstm\": \n","            output, (hidden,cell) = self.step(embedded, (hidden,cell))\n","            return output, (hidden,cell)\n","\n","    def initHidden(self):\n","        #for bidirection\n","        num_layers=self.num_layers\n","        if self.bidirectional:\n","            num_layers=self.num_layers*2\n","        hidden=torch.zeros(num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","        if self.typecell==\"lstm\":\n","            cell=torch.zeros(num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","            return (hidden,cell)        \n","        return hidden\n","\n","\n","        \n","class Attention(nn.Module):\n","    def __init__(self,input_size,hidden_size,embedding_size,num_layers,output_size,typecell,dropout):\n","        super(Attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(0.2)\n","        self.num_layers=num_layers\n","        self.output_size=output_size\n","        self.typecell=typecell\n","        self.p=dropout\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.u=nn.Linear(hidden_size,hidden_size)\n","        self.w=nn.Linear(hidden_size,hidden_size)\n","        self.v=nn.Linear(hidden_size,1)\n","        if self.typecell==\"gru\":\n","            self.step = nn.GRU(hidden_size+embedding_size, hidden_size,num_layers,dropout=self.p)   \n","        if self.typecell==\"rnn\":\n","            self.step = nn.RNN(hidden_size+embedding_size, hidden_size,num_layers,dropout=self.p)   \n","        if self.typecell==\"lstm\":\n","            self.step = nn.LSTM(hidden_size+embedding_size, hidden_size,num_layers,dropout=self.p) \n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.Softmax(dim=2)\n","        \n","    def forward(self, inp, hidden,encoder_output,cell=None):\n","        embedded = self.embedding(inp)\n","        u1=self.u(encoder_output)\n","        w1=self.w(hidden[-1])\n","        z=nn.Tanh()(u1+w1.resize(1,BATCH_SIZE,self.hidden_size))\n","        et=self.v(z)\n","        alpha=nn.Softmax(dim=0)(et)\n","        ct=torch.einsum(\"snk,snl->knl\", alpha, encoder_output)\n","        ctet=torch.cat((ct,embedded),dim=2)\n","        if self.typecell==\"gru\":\n","            output, hidden = self.step(ctet, hidden)  \n","            output1=self.out(output)\n","            return output1,hidden,alpha\n","        if self.typecell==\"rnn\":\n","            output, hidden = self.step(ctet, hidden)  \n","            output1=self.out(output)\n","            return output1,hidden,alpha\n","        if self.typecell==\"lstm\": \n","            output, (hidden,cell) = self.step(ctet,  (hidden,cell))\n","            output1=self.out(output)\n","            return output1, (hidden,cell),alpha\n","\n","    \n","    def initHidden(self):\n","        num_layers=self.num_layers\n","        hidden=torch.zeros(num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","        if self.typecell==\"lstm\":\n","            cell=torch.zeros(num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","            return (hidden,cell)        \n","        return hidden\n","\n","    \n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder,hencoder,cell=None):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.hencoder=hencoder\n","        self.cell=cell\n","    def forward(self, inp, target,teacher_force_ratio,capture=True):\n","        outputs = torch.zeros(HINDILEN,BATCH_SIZE ,len(hindivocab)).to(device)\n","        if self.encoder.typecell==\"lstm\":\n","            p,(hencoder3d,cell)=self.encoder.forward(inp.to(device),self.hencoder,self.cell)\n","        else:\n","            p,hencoder3d=self.encoder.forward(inp.to(device),self.hencoder)\n","        p=torch.split(p,[self.encoder.hidden_size,self.encoder.hidden_size],dim=2)\n","        p=torch.add(p[0],p[1])/2\n","        tempdecoder=torch.zeros(self.encoder.num_layers,BATCH_SIZE,hencoder3d.size()[2]).to(device)\n","        tempdecoder[0]=hencoder3d[hencoder3d.size()[0]//2-1]\n","        tempdecoder[1]=hencoder3d[(hencoder3d.size()[0]//2)*2-1]\n","        hdecoder=torch.add(tempdecoder[0],tempdecoder[1])/2\n","        hdecoder=hdecoder.repeat(self.decoder.num_layers,1,1)\n","        if self.encoder.typecell==\"lstm\":\n","            tempcell=torch.zeros(2,BATCH_SIZE,cell.size()[2]).to(device)\n","            tempcell[0]=hencoder3d[cell.size()[0]//2-1]\n","            tempcell[1]=hencoder3d[(cell.size()[0]//2)*2-1]\n","            cell=torch.add(tempcell[0],tempcell[1])/2\n","            cell=cell.repeat(self.decoder.num_layers,1,1)\n","        x=torch.full((1,BATCH_SIZE),hindidictc['0'])\n","        g=[]\n","        if self.encoder.typecell==\"lstm\":\n","            output,(hdecoder,cell),alpha=self.decoder.forward(x.to(device),hdecoder,p.to(device),cell)\n","            if capture:\n","                g.append(alpha)\n","        else:\n","            output,hdecoder,alpha=self.decoder.forward(x.to(device),hdecoder,p.to(device))\n","            g.append(alpha)\n","        outputs[0]=output\n","        t=1\n","        for i in range(1,HINDILEN):\n","            if random.random() > teacher_force_ratio:\n","                output=self.decoder.softmax(output)\n","                nextinp=torch.argmax(output, dim=2)\n","                if self.encoder.typecell==\"lstm\":\n","                    output,(hdecoder,cell),alpha=self.decoder.forward(nextinp.to(device),hdecoder,p.to(device),cell)\n","                    if capture:\n","                        g.append(alpha)                    \n","                    \n","                else:\n","                    output,hdecoder,alpha=self.decoder.forward(nextinp.to(device),hdecoder,p.to(device))\n","                    g.append(alpha)\n","                outputs[t]=output\n","                t+=1\n","            else:\n","                nextinp=target[i-1,:].unsqueeze(0)\n","                if self.encoder.typecell==\"lstm\":\n","                    output,(hdecoder,cell),alpha=self.decoder.forward(nextinp.to(device),hdecoder,p.to(device),cell)\n","                    if capture:\n","                        g.append(alpha)  \n","                else:\n","                    output,hdecoder,alpha=self.decoder.forward(nextinp.to(device),hdecoder,p.to(device))\n","                    g.append(alpha)\n","                outputs[t]=output\n","                t+=1\n","        return outputs,g\n","        \n","    \n","alpha=[]\n","def train(encoder,decoder,seq2seq,epoch):\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","    loss=0\n","    count=0\n","    numbatches=englishwords.shape[0]//BATCH_SIZE\n","    for ep in range(epoch):\n","        trainloss=0\n","        train_correct=0\n","        for i in range(numbatches):\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            temp=englishwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temph=hindiwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temp=temp.t()\n","            temph=temph.t()\n","            global alpha\n","            output,alpha=seq2seq.forward(temp,temph,0.5)\n","            train_correct+=train_accuracy(output,temph)\n","            output = output[:].reshape(-1, output.shape[2])\n","            tem = temph[:].reshape(-1)\n","            loss=criterion(output,tem)\n","            loss.backward()\n","            trainloss+=loss.item()\n","            torch.nn.utils.clip_grad_norm_(decoder.parameters(),max_norm = 1)\n","            torch.nn.utils.clip_grad_norm_(encoder.parameters(),max_norm = 1)\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","        val_correct,cur_loss=accuracy(seq2seq,englishwordsval,hindiwordsval)\n","        print(ep,trainloss/(51200*HINDILEN),cur_loss/(4096*HINDILEN),val_correct,train_correct)\n","        trainloss=trainloss/(51200*HINDILEN)\n","        cur_loss=cur_loss/(4096*HINDILEN)\n","        tra_acc=train_correct/51200\n","        val_acc=val_correct/4096\n","        wandb.log({\"Training loss\":trainloss,'Val loss':cur_loss,'Training Accuracy':tra_acc,'Val Accuracy':val_acc})\n","\n","def train_accuracy(output,temph):\n","        output=nn.Softmax(dim=2)(output)\n","        output=torch.argmax(output,dim=2)\n","        temph=temph.t()\n","        output=output.t()\n","        correct=0\n","        for i in range(BATCH_SIZE):\n","            if(torch.equal(output[i],temph[i])):\n","                correct+=1\n","        return correct\n","        \n","def accuracy(seq2seq,english,hindi):\n","    numbatches=english.shape[0]//BATCH_SIZE\n","    correct=0\n","    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","    loss=0\n","    for i in range(numbatches):\n","        temp=english[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temph=hindi[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temp=temp.t()\n","        temph=temph.t()\n","        global alpha\n","        output,alpha=seq2seq.forward(temp,temph,0)\n","        o = output[:].reshape(-1, output.shape[2])\n","        tem = temph[:].reshape(-1)\n","        x=criterion(o,tem)\n","        loss+=x.item()\n","        output=nn.Softmax(dim=2)(output)\n","        output=torch.argmax(output,dim=2)\n","        temph=temph.t()\n","        output=output.t()\n","        for i in range(BATCH_SIZE):\n","            if(torch.equal(output[i],temph[i])):\n","                correct+=1\n","\n","    return correct,loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Saving File for predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy(seq2seq,englishwordstest,hindiwordstest)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def filetest(seq2seq,english,hindi):\n","    numbatches=english.shape[0]//BATCH_SIZE\n","    correct=0\n","    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","    loss=0\n","    for i in range(numbatches):\n","        temp=english[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temph=hindi[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temp=temp.t()\n","        temph=temph.t()\n","        output,alpha=seq2seq.forward(temp,temph,0)\n","        o = output[:].reshape(-1, output.shape[2])\n","        tem = temph[:].reshape(-1)\n","        x=criterion(o,tem)\n","        loss+=x.item()\n","        output=nn.Softmax(dim=2)(output)\n","        output=torch.argmax(output,dim=2)\n","        temph=temph.t()\n","        output=output.t()\n","        temp=temp.t()\n","        for i in range(BATCH_SIZE):\n","            eng=[]\n","            for y in temp[i]:\n","                if y==2:\n","                    break\n","                eng.append(englishdicti[y.item()])\n","            hindip=[]\n","            for y in output[i]:\n","                if y==1:\n","                    break\n","                hindip.append(hindidicti[y.item()])       \n","            hinditrue=[]\n","            for y in temph[i]:\n","                if y==1:\n","                    break\n","                hinditrue.append(hindidicti[y.item()]) \n","            hindip=''.join(hindip)\n","            hinditrue=''.join(hinditrue)\n","            eng=''.join(eng)\n","            with open('output4.txt', 'a', encoding='utf-8') as file:\n","                file.write(eng+' '+hinditrue+' '+hindip+'\\n')\n","            if(torch.equal(output[i],temph[i])):\n","                correct+=1\n","\n","    return correct,loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filetest(seq2seq,englishwordstest,hindiwordstest)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Attention Heatmap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy(seq2seq,englishwordstest[:128],hindiwordstest[:128])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.font_manager import FontProperties\n","fig,axes=plt.subplots(3, 3, figsize=(12, 12))\n","figsize=(64,64)\n","fig.tight_layout(pad=5.0)\n","fig.subplots_adjust(top=0.90)\n","\n","axes = axes.ravel()\n","def plot_rowwise_heatmap(matrix,eng,hindi,i):\n","    rows, cols = matrix.shape\n","\n","    # Create a figure and axis\n","\n","    # Set the colormap\n","    cmap = sns.color_palette(\"viridis\")\n","\n","    # Plot the heatmap\n","    sns.heatmap(matrix, cmap=cmap,  ax=axes[i], cbar=True, cbar_kws={'label': 'Value'})\n","    hindi_font = FontProperties(fname = os.getcwd()+'/nirmala.ttf')\n","\n","    axes[i].set_xticks(np.arange(cols) + 0.5)\n","    axes[i].set_xticklabels(hindi,fontproperties=hindi_font)\n","\n","\n","    axes[i].set_yticks(np.arange(rows) + 0.5)\n","    axes[i].set_yticklabels(eng)\n","\n","    # Add a colorbar\n","    cbar = axes[i].collections[0].colorbar\n","    cbar.set_label('Value')\n","\n","\n","\n","alpha1=torch.cat(tuple(x for x in alpha),dim = 2).to(device)\n","alpha2=alpha1.permute(1,0,2)\n","# print(alpha2[0].size())\n","x=alpha2[0].cpu().detach().numpy()\n","plot=[]\n","for i in range(9):\n","    eng=[]\n","    for y in englishwordstest[i]:\n","        if y==2:\n","            break\n","        eng.append(englishdicti[y.item()])\n","    hindi=[]\n","    for y in hindiwordstest[i]:\n","        if y==1:\n","            break\n","        hindi.append(hindidicti[y.item()])\n","    plot_rowwise_heatmap(x[0:len(eng),0:len(hindi)],eng,hindi,i)\n","    \n","\n","fig.suptitle('Attention Mechanism(HeatMap)',fontsize=20)\n","plt.savefig('plot.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Sweep code for Attention"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sweep():\n","    wandb.init(project='DLAssignment3')\n","    config = wandb.config\n","    wandb.run.name = \"Attn_cellType_{}_embSize_{}_layers_{}_batchSize_{}_hidden_{}_dropout_{}\".format(config.cell_type,config.input_embedding_size,config.no_of_layers,config.batchsize,config.hidden_size,config.dropout)\n","    hidden_size = config.hidden_size\n","    char_embed_size = config.input_embedding_size\n","    no_of_layers = config.no_of_layers\n","    epochs = config.epochs\n","    batchsize = config.batchsize\n","    dropout = config.dropout\n","    cell_type=config.cell_type\n","    if(config.bidirectional=='Yes'):\n","        bidirectional=True\n","    else:\n","        bidirectional=False\n","    encoder=EncoderRNN(len(englishvocab),hidden_size,char_embed_size,no_of_layers,cell_type,dropout,bidirectional).to(device)\n","    decoder=Attention(len(hindivocab),hidden_size,char_embed_size,no_of_layers,len(hindivocab),cell_type,dropout).to(device)\n","    if cell_type=='lstm':\n","        hencoder,cell=encoder.initHidden()\n","        seq2seq=Seq2Seq(encoder,decoder,hencoder,cell)\n","    else:\n","        hencoder=encoder.initHidden()\n","        seq2seq=Seq2Seq(encoder,decoder,hencoder)\n","    train(encoder,decoder,seq2seq,epochs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_configuration = {\n","    'method' : 'bayes',\n","    'metric' : { 'goal' : 'maximize',\n","    'name' : 'Val Accuracy'},\n","    'parameters':{\n","        'batchsize' : {'values' : [128]},\n","        'input_embedding_size' : {'values' : [256,128,512]},\n","        'no_of_layers' : {'values' : [2,3,4,5]},\n","        'hidden_size' : {'values' : [256,128,512]},\n","        'cell_type' : {'values' : ['lstm','gru','rnn']},\n","        'bidirectional' : {'values' : ['Yes']},\n","        'dropout' : {'values' : [0.2,0.4]},\n","        'epochs' : {'values' : [15,20]}\n","    }\n","}\n","sweep_id = wandb.sweep(sweep = sweep_configuration,project = 'DLAssignment3')\n","wandb.agent(sweep_id,function=sweep,count)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
