{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import random\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, csv_file):\n","        self.data = pd.read_csv(csv_file,names=[\"English\",\"Hindi\"],header=None)\n","        \n","    def __getitem__(self, index):\n","        x = self.data.iloc[index][\"English\"]\n","        y = self.data.iloc[index][\"Hindi\"]\n","        return x, y\n","    \n","    def __len__(self):\n","        return len(self.data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = MyDataset('/kaggle/input/transliteration/hin_train.csv')\n","train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n","test_data = MyDataset('/kaggle/input/transliteration/hin_test.csv')\n","test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)\n","val_data = MyDataset('/kaggle/input/transliteration/hin_valid.csv')\n","val_dataloader = DataLoader(val_data, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["51200\n"]}],"source":["print(len(train_data))\n","ENGLEN=32\n","HINDILEN=32\n","BATCH_SIZE=128\n","englishwords=torch.full((len(train_data), ENGLEN), 2).to(device)\n","hindiwords=torch.full((len(train_data), HINDILEN), 2).to(device)\n","# hindivocab=[chr(i) for i in range(2304, 2432)]\n","# print(hindivocab.sort())\n","# print(hindivocab)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['0', '1', '2', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"]}],"source":["hindivocab=set()\n","englishvocab=set()\n","for x,y in train_data:\n","    for letter in x:\n","        englishvocab.add(letter)\n","    for letter in y:\n","        hindivocab.add(letter)  \n","for x,y in test_data:\n","    for letter in x:\n","        englishvocab.add(letter)\n","    for letter in y:\n","        hindivocab.add(letter)\n","for x,y in test_data:\n","    for letter in x:\n","        englishvocab.add(letter)\n","    for letter in y:\n","        hindivocab.add(letter)\n","hindivocab=list(hindivocab)\n","hindivocab.sort()\n","englishvocab=list(englishvocab)\n","englishvocab.sort()\n","hindivocab.insert(0,'0')#start\n","hindivocab.insert(1,'1') #end\n","hindivocab.insert(2,'2') #pad\n","englishvocab.insert(0,'0')#start\n","englishvocab.insert(1,'1') #end\n","englishvocab.insert(2,'2') #pad\n","print(englishvocab)\n","hindidictc={}\n","englishdictc={}\n","hindidicti={}\n","englishdicti={}\n","for i in range(len(hindivocab)):\n","    hindidicti[i]=hindivocab[i]\n","    hindidictc[hindivocab[i]]=i\n","for i in range(len(englishvocab)):\n","    englishdicti[i]=englishvocab[i]\n","    englishdictc[englishvocab[i]]=i\n","\n","c=0\n","for x,y in train_data:\n","    for i in range(len(x)):\n","        englishwords[c][i]=englishdictc[x[i]]\n","    for i in range(len(y)):\n","        hindiwords[c][i]=hindidictc[y[i]]\n","    hindiwords[c][i+1]=1\n","    c+=1\n","\n","englishwordsval=torch.full((len(val_data), ENGLEN), 2).to(device)\n","hindiwordsval=torch.full((len(val_data), HINDILEN), 2).to(device)\n","c=0\n","for x,y in test_data:\n","    for i in range(len(x)):\n","        englishwordsval[c][i]=englishdictc[x[i]]\n","    for i in range(len(y)):\n","        hindiwordsval[c][i]=hindidictc[y[i]]\n","    hindiwordsval[c][i+1]=1\n","    c+=1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["68\n"]}],"source":["print(len(hindivocab))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size,hidden_size,embedding_size,num_layers):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(0.2)\n","        self.num_layers=num_layers\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.gru = nn.GRU(embedding_size, hidden_size,num_layers,dropout=0.2,bidirectional=True)\n","\n","    def forward(self, inp, hidden):\n","        embedded = self.dropout(self.embedding(inp))\n","        output, hidden = self.gru(embedded, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        #for bidirection\n","        return torch.zeros(2*self.num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","\n","    \n","class DecoderRNN(nn.Module):\n","    def __init__(self,input_size,hidden_size,embedding_size,num_layers,output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(0.2)\n","        self.num_layers=num_layers\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.gru = nn.GRU(embedding_size,hidden_size,num_layers,dropout=0.2)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.Softmax(dim=2)\n","\n","    def forward(self, inp, hidden):\n","        embedded = self.dropout(self.embedding(inp))\n","        output, hidden = self.gru(embedded, hidden)\n","        output1=self.out(output)\n","        return output1, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(self.num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","            \n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder,hencoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.hencoder=hencoder\n","    def forward(self, inp, target,teacher_force_ratio):\n","        outputs = torch.zeros(HINDILEN,BATCH_SIZE ,len(hindivocab)).to(device)\n","        p,hencoder3d=self.encoder.forward(inp.to(device),self.hencoder)   \n","        tempdecoder=torch.zeros(2,BATCH_SIZE,hencoder3d.size()[2]).to(device)\n","        tempdecoder[0]=hencoder3d[hencoder3d.size()[0]//2-1]\n","        tempdecoder[1]=hencoder3d[(hencoder3d.size()[0]//2)*2-1]\n","        hdecoder=tempdecoder.mean(dim=0)\n","        hdecoder=hdecoder.repeat(self.decoder.num_layers,1,1)\n","#         print(p.size())\n","#         hencoder4d=hencoder3d.view(hencoder3d.size()[0]//2,hencoder3d.size()[0]//2,hencoder3d.size()[1],hencoder3d.size()[2])\n","#         hdecoder=self.decoder.initHidden()\n","#         for it in range(hencoder3d.size()//2):\n","#             hdecoder[it]=hencoder4d[it].mean(dim=0)\n","        x=torch.full((1,BATCH_SIZE),hindidictc['0'])\n","        output,hdecoder=self.decoder.forward(x.to(device),hdecoder)\n","        outputs[0]=output\n","        t=1\n","        if random.random() > teacher_force_ratio:\n","            for i in range(1,HINDILEN):\n","                output=self.decoder.softmax(output)\n","                nextinp=torch.argmax(output, dim=2)\n","                output,hdecoder=self.decoder.forward(nextinp.to(device),hdecoder)\n","                outputs[t]=output\n","                t+=1\n","        else:            \n","            for i in range(1,HINDILEN):\n","                nextinp=target[i-1,:].unsqueeze(0)\n","                output,hdecoder=self.decoder.forward(nextinp.to(device),hdecoder)\n","                outputs[t]=output\n","                t+=1\n","        return outputs\n","        \n","    \n","\n","def train(encoder,decoder,seq2seq):\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","    loss=0\n","    count=0\n","    numbatches=englishwords.shape[0]//BATCH_SIZE\n","    for ep in range(5):\n","        trainloss=0\n","        for i in range(numbatches):\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            temp=englishwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temph=hindiwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temp=temp.t()\n","            temph=temph.t()\n","            output=seq2seq.forward(temp,temph,0.5)\n","            output = output[:].reshape(-1, output.shape[2])\n","            tem = temph[:].reshape(-1)\n","            loss=criterion(output,tem)\n","            loss.backward()\n","            trainloss+=loss.item()\n","            torch.nn.utils.clip_grad_norm_(decoder.parameters(),max_norm = 1)\n","            torch.nn.utils.clip_grad_norm_(encoder.parameters(),max_norm = 1)\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","        train_correct=accuracy(seq2seq,englishwords,hindiwords)\n","        val_correct=accuracy(seq2seq,englishwordsval,hindiwordsval)\n","        print(ep,trainloss/(51200*HINDILEN),train_correct,val_correct)\n","    \n","def accuracy(seq2seq,english,hindi):\n","    numbatches=english.shape[0]//BATCH_SIZE\n","    correct=0\n","    for i in range(numbatches):\n","        temp=english[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temph=hindi[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temp=temp.t()\n","        temph=temph.t()\n","        output=seq2seq.forward(temp,temph,0)\n","        output=nn.Softmax(dim=2)(output)\n","        output=torch.argmax(output,dim=2)\n","        temph=temph.t()\n","        output=output.t()\n","        for i in range(BATCH_SIZE):\n","            if(torch.equal(output[i],temph[i])):\n","                correct+=1\n","    return correct\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0 0.5310823743976653 5969 608\n","1 0.3353901833668351 10580 927\n","2 0.29768058070912956 11915 968\n","3 0.274017201801762 14529 1121\n","4 0.25678213089704516 14455 1067\n"]}],"source":["encoder=EncoderRNN(len(englishvocab),256,256,2).to(device)\n","decoder=DecoderRNN(len(hindivocab),256,256,2,len(hindivocab)).to(device)\n","hencoder=encoder.initHidden()\n","seq2seq=Seq2Seq(encoder,decoder,hencoder)\n","train(encoder,decoder,seq2seq)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def forward( inp, target,teacher_force_ratio=0.5):\n","        outputs = torch.zeros(HINDILEN,BATCH_SIZE ,len(hindivocab)).to(device)\n","        _,hdecoder=encoder.forward(inp.to(device),hencoder)        \n","        x=torch.full((1,BATCH_SIZE),hindidictc['0'])\n","        output,hdecoder=decoder.forward(x.to(device),hdecoder)\n","        outputs[0]=output\n","        t=1\n","        for i in range(1,HINDILEN):\n","                output=decoder.softmax(output)\n","                nextinp=torch.argmax(output, dim=2)\n","                output,hdecoder=decoder.forward(nextinp.to(device),hdecoder)\n","                outputs[t]=output\n","                t+=1\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def forwardbi( inp, target,teacher_force_ratio=0.5):\n","        outputs = torch.zeros(HINDILEN,BATCH_SIZE ,len(hindivocab)).to(device)\n","        _,hencoder3d=encoder.forward(inp.to(device),hencoder)   \n","        tempdecoder=torch.zeros(2,BATCH_SIZE,hencoder3d.size()[2]).to(device)\n","        tempdecoder[0]=hencoder3d[hencoder3d.size()[0]//2-1]\n","        tempdecoder[1]=hencoder3d[(hencoder3d.size()[0]//2)*2-1]\n","        hdecoder=tempdecoder.mean(dim=0)\n","        hdecoder=hdecoder.repeat(decoder.num_layers,1,1)       \n","        x=torch.full((1,BATCH_SIZE),hindidictc['0'])\n","        output,hdecoder=decoder.forward(x.to(device),hdecoder)\n","        outputs[0]=output\n","        t=1\n","        for i in range(1,HINDILEN):\n","                output=decoder.softmax(output)\n","                nextinp=torch.argmax(output, dim=2)\n","                output,hdecoder=decoder.forward(nextinp.to(device),hdecoder)\n","                outputs[t]=output\n","                t+=1\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["17597\n"]}],"source":["def accuracy(englishwords,hindiwords):\n","    numbatches=englishwords.shape[0]//BATCH_SIZE\n","    correct=0\n","    for i in range(numbatches):\n","        temp=englishwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temph=hindiwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","        temp=temp.t()\n","        temph=temph.t()\n","        output=forwardbi(temp,temph)\n","        output=nn.Softmax(dim=2)(output)\n","        output=torch.argmax(output,dim=2)\n","        temph=temph.t()\n","        output=output.t()\n","        for i in range(BATCH_SIZE):\n","            if(torch.equal(output[i],temph[i])):\n","                correct+=1\n","        return correct\n","\n","print(correct)\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["englishwordsval=torch.full((len(val_data), ENGLEN), 2).to(device)\n","hindiwordsval=torch.full((len(val_data), HINDILEN), 2).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["c=0\n","for x,y in test_data:\n","    for i in range(len(x)):\n","        englishwordsval[c][i]=englishdictc[x[i]]\n","    for i in range(len(y)):\n","        hindiwordsval[c][i]=hindidictc[y[i]]\n","    hindiwordsval[c][i+1]=1\n","    c+=1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1291\n"]}],"source":["numbatches=englishwordsval.shape[0]//BATCH_SIZE\n","correct=0\n","for i in range(numbatches):\n","    temp=englishwordsval[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","    temph=hindiwordsval[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","    temp=temp.t()\n","    temph=temph.t()\n","    output=forwardbi(temp,temph)\n","    output=nn.Softmax(dim=2)(output)\n","    output=torch.argmax(output,dim=2)\n","    temph=temph.t()\n","    output=output.t()\n","    for i in range(BATCH_SIZE):\n","        if(torch.equal(output[i],temph[i])):\n","            correct+=1\n","\n","\n","print(correct)\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4095\n"]}],"source":["# 32,128,512\n","# max size,bs,2*layer(1)*hidden"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder,hencoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.hencoder=hencoder\n","    def forward(self, inp, target,teacher_force_ratio=0.5):\n","        outputs = torch.zeros(HINDILEN,BATCH_SIZE ,len(hindivocab)).to(device)\n","        p,hencoder3d=self.encoder.forward(inp.to(device),self.hencoder)   \n","        tempdecoder=torch.zeros(2,BATCH_SIZE,hencoder3d.size()[2]).to(device)\n","        tempdecoder[0]=hencoder3d[hencoder3d.size()[0]//2-1]\n","        tempdecoder[1]=hencoder3d[(hencoder3d.size()[0]//2)*2-1]\n","        hdecoder=tempdecoder.mean(dim=0)\n","        hdecoder=hdecoder.repeat(self.decoder.num_layers,1,1)\n","        x=torch.full((1,BATCH_SIZE),hindidictc['0'])\n","        output,hdecoder=self.decoder.forward(x.to(device),hdecoder,p.to(device))\n","#         print(hdecoder.size())\n","        outputs[0]=output\n","        t=1\n","        if random.random() > teacher_force_ratio:\n","            for i in range(1,HINDILEN):\n","                output=self.decoder.softmax(output)\n","                nextinp=torch.argmax(output, dim=2)\n","                output,hdecoder=self.decoder.forward(nextinp.to(device),hdecoder,p.to(device))\n","                outputs[t]=output\n","                t+=1\n","        else:            \n","            for i in range(1,HINDILEN):\n","                nextinp=target[i-1,:].unsqueeze(0)\n","                output,hdecoder=self.decoder.forward(nextinp.to(device),hdecoder,p.to(device))\n","                outputs[t]=output\n","                t+=1\n","        return outputs\n","    \n","class Attention(nn.Module):\n","    def __init__(self,input_size,hidden_size,embedding_size,num_layers,output_size):\n","        super(Attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(0.2)\n","        self.num_layers=num_layers\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.u=nn.Linear(hidden_size*2,hidden_size)\n","        self.w=nn.Linear(hidden_size,hidden_size)\n","        self.v=nn.Linear(hidden_size,1)\n","        self.gru = nn.GRU(2*hidden_size+embedding_size,hidden_size,num_layers,dropout=0.2)\n","        #first param will change if bidir is false\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.Softmax(dim=2)\n","    def forward(self, inp, hidden,encoder_output):\n","        hidden_r=hidden.mean(dim=0)\n","        hidden_r=hidden_r.repeat(HINDILEN,1,1)\n","        embedded = self.dropout(self.embedding(inp))\n","#         print(encoder_output.size())\n","        u1=self.u(encoder_output)\n","        w1=self.w(hidden_r)\n","        z=nn.Tanh()(u1+w1)\n","        et=self.v(z)\n","#         print(et)\n","        alpha=nn.Softmax(dim=0)(et)\n","#         print(alpha)\n","        ct = torch.sum(alpha * encoder_output, dim=0, keepdim=True)\n","        ctet=torch.cat((ct,embedded),dim=2)\n","        output,hidden=self.gru(ctet,hidden)\n","        output1=self.out(output)\n","#         print(output1.size(),hidden.size())\n","        return output1, hidden\n","    def initHidden(self):\n","        return torch.zeros(self.num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","\n","        \n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"data":{"text/plain":["(tensor([[[ 0.0354,  0.3127,  0.2832,  ...,  0.0775,  0.0958,  0.1181],\n","          [-0.0638,  0.3168,  0.3439,  ..., -0.0098,  0.2600,  0.1728],\n","          [ 0.0404,  0.2177,  0.2104,  ...,  0.1613,  0.0616,  0.2749],\n","          ...,\n","          [ 0.0877,  0.2104,  0.3672,  ...,  0.2486, -0.0261,  0.2160],\n","          [-0.0459,  0.2812,  0.1697,  ...,  0.1973,  0.1747,  0.3285],\n","          [ 0.0590,  0.2805,  0.1684,  ...,  0.1164,  0.0374,  0.1961]]],\n","        grad_fn=<ViewBackward0>),\n"," tensor([[[ 0.2258, -0.2521, -0.4574,  ...,  0.1004,  0.2347,  0.2183],\n","          [-0.1059, -0.5455, -0.0390,  ...,  0.0606,  0.2736,  0.3108],\n","          [ 0.2226, -0.4079, -0.3475,  ...,  0.1928,  0.1755,  0.3311],\n","          ...,\n","          [ 0.1606, -0.2324, -0.2096,  ...,  0.0621,  0.2954,  0.0214],\n","          [-0.1228, -0.1411, -0.2231,  ...,  0.2590,  0.2142, -0.1669],\n","          [-0.1208,  0.1471, -0.3834,  ...,  0.0186,  0.2418,  0.1455]]],\n","        grad_fn=<StackBackward0>))"]},"metadata":{},"output_type":"display_data"}],"source":["inp=torch.full((1,BATCH_SIZE),hindidictc['0']).to(torch.int32)\n","hidden=torch.full((1,BATCH_SIZE,256),hindidictc['0']).to(torch.float32)#to be repeated bs times in code\n","encoder_outputs=torch.full((32,BATCH_SIZE,256*2),hindidictc['0']).to(torch.float32)\n","attention=Attention(len(hindivocab),256,256,2,len(hindivocab))\n","attention.forward(inp,hidden,encoder_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0 0.6984311667829752\n","1 0.431121762804687\n","2 0.3665090965479612\n","3 0.33561007618904115\n","4 0.31535186517983677\n","5 0.295670186560601\n","6 0.2784927454032004\n","7 0.2676304139196873\n","8 0.25241304429247974\n","9 0.2399233009573072\n","10 0.2382461895979941\n","11 0.22860895477235318\n","12 0.22379849020391704\n","13 0.21717242494225503\n","14 0.2138343177642673\n"]}],"source":["encoder=EncoderRNN(len(englishvocab),128,128,2).to(device)\n","decoder=Attention(len(hindivocab),128,128,2,len(hindivocab)).to(device)\n","hencoder=encoder.initHidden()\n","seq2seq=Seq2Seq(encoder,decoder,hencoder)\n","def train():\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n","    loss=0\n","    count=0\n","    numbatches=englishwords.shape[0]//BATCH_SIZE\n","    for ep in range(15):\n","        trainloss=0\n","        for i in range(numbatches):\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            temp=englishwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temph=hindiwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temp=temp.t()\n","            temph=temph.t()\n","            output=seq2seq.forward(temp,temph,0.5)\n","            output = output[:].reshape(-1, output.shape[2])\n","            tem = temph[:].reshape(-1)\n","            loss=criterion(output,tem)\n","            loss.backward()\n","            trainloss+=loss.item()\n","            torch.nn.utils.clip_grad_norm_(decoder.parameters(),max_norm = 1)\n","            torch.nn.utils.clip_grad_norm_(encoder.parameters(),max_norm = 1)\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","        print(ep,trainloss/(51200*HINDILEN))\n","    \n","\n","train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def forwardbi( inp, target,teacher_force_ratio=0.5):\n","        outputs = torch.zeros(HINDILEN,BATCH_SIZE ,len(hindivocab)).to(device)\n","        p,hencoder3d=encoder.forward(inp.to(device),hencoder)   \n","        tempdecoder=torch.zeros(2,BATCH_SIZE,hencoder3d.size()[2]).to(device)\n","        tempdecoder[0]=hencoder3d[hencoder3d.size()[0]//2-1]\n","        tempdecoder[1]=hencoder3d[(hencoder3d.size()[0]//2)*2-1]\n","        hdecoder=tempdecoder.mean(dim=0)\n","        hdecoder=hdecoder.repeat(decoder.num_layers,1,1)       \n","        x=torch.full((1,BATCH_SIZE),hindidictc['0'])\n","        output,hdecoder=decoder.forward(x.to(device),hdecoder,p)\n","        outputs[0]=output\n","        t=1\n","        for i in range(1,HINDILEN):\n","                output=decoder.softmax(output)\n","                nextinp=torch.argmax(output, dim=2)\n","                output,hdecoder=decoder.forward(nextinp.to(device),hdecoder,p)\n","                outputs[t]=output\n","                t+=1\n","        return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
