{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import random\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import random\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, csv_file):\n","        self.data = pd.read_csv(csv_file)\n","        \n","    def __getitem__(self, index):\n","        x = self.data.iloc[index,0]\n","        y = self.data.iloc[index,1]\n","        return x, y\n","    \n","    def __len__(self):\n","        return len(self.data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = MyDataset('/kaggle/input/transliteration/hin_train.csv')\n","train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n","test_data = MyDataset('/kaggle/input/transliteration/hin_test.csv')\n","test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)\n","val_data = MyDataset('/kaggle/input/transliteration/hin_valid.csv')\n","val_dataloader = DataLoader(val_data, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["51199\n"]}],"source":["print(len(train_data))\n","ENGLEN=32\n","HINDILEN=32\n","BATCH_SIZE=256\n","englishwords=torch.full((len(train_data), ENGLEN), 2).to(device)\n","hindiwords=torch.full((len(train_data), HINDILEN), 2).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['0', '1', '2', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"]}],"source":["hindivocab=set()\n","englishvocab=set()\n","for x,y in train_data:\n","    for letter in x:\n","        englishvocab.add(letter)\n","    for letter in y:\n","        hindivocab.add(letter)  \n","        \n","hindivocab=list(hindivocab)\n","hindivocab.sort()\n","englishvocab=list(englishvocab)\n","englishvocab.sort()\n","hindivocab.insert(0,'0')#start\n","hindivocab.insert(1,'1') #end\n","hindivocab.insert(2,'2') #pad\n","englishvocab.insert(0,'0')#start\n","englishvocab.insert(1,'1') #end\n","englishvocab.insert(2,'2') #pad\n","print(englishvocab)\n","hindidictc={}\n","englishdictc={}\n","hindidicti={}\n","englishdicti={}\n","for i in range(len(hindivocab)):\n","    hindidicti[i]=hindivocab[i]\n","    hindidictc[hindivocab[i]]=i\n","for i in range(len(englishvocab)):\n","    englishdicti[i]=englishvocab[i]\n","    englishdictc[englishvocab[i]]=i\n","\n","c=0\n","for x,y in train_data:\n","    for i in range(len(x)):\n","        englishwords[c][i]=englishdictc[x[i]]\n","    for i in range(len(y)):\n","        hindiwords[c][i]=hindidictc[y[i]]\n","    hindiwords[c][i+1]=1\n","    c+=1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["67\n"]}],"source":["print(len(hindivocab))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.024784395671304082\n","0.015222961323161144\n","0.012326647563895676\n","0.011098967183897912\n","0.01042161824352661\n"]}],"source":["# temp=torch.full((32,16), 2).to(device)\n","# temph=torch.full((64, 16), 2).to(device)\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size,hidden_size,embedding_size,num_layers):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(0.8)\n","        self.num_layers=num_layers\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.gru = nn.GRU(embedding_size, hidden_size,num_layers,dropout=0.8)\n","\n","    def forward(self, inp, hidden):\n","        embedded = self.dropout(self.embedding(inp))\n","        output, hidden = self.gru(embedded, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(self.num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","\n","    \n","class DecoderRNN(nn.Module):\n","    def __init__(self,input_size,hidden_size,embedding_size,num_layers,output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.dropout = nn.Dropout(0.8)\n","        self.num_layers=num_layers\n","        self.embedding = nn.Embedding(input_size, embedding_size)\n","        self.gru = nn.GRU(embedding_size,hidden_size,num_layers,dropout=0.8)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.Softmax(dim=2)\n","\n","    def forward(self, inp, hidden):\n","        embedded = self.dropout(self.embedding(inp))\n","        output, hidden = self.gru(embedded, hidden)\n","        output1=self.out(output)\n","#         print(output1)\n","#         output2 =self.softmax(output1)\n","        return output1, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(self.num_layers,BATCH_SIZE,self.hidden_size, device=device)\n","            \n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","    def forward(self, inp, target,teacher_force_ratio=0.5):\n","        outputs = torch.zeros(HINDILEN,BATCH_SIZE ,len(hindivocab)).to(device)\n","        hencoder=self.encoder.initHidden()\n","        _,hencoder=self.encoder.forward(inp.to(device),hencoder)        \n","        x=torch.full((1,BATCH_SIZE),hindidictc['0'])\n","        output,hdecoder=self.decoder.forward(x.to(device),hencoder)\n","        outputs[0]=output\n","        t=1\n","        teacher_forcing_ratio=0.5\n","        if random.random() < teacher_forcing_ratio:\n","            for i in range(1,HINDILEN):\n","                output=self.decoder.softmax(output)\n","                nextinp=torch.argmax(output, dim=2)\n","                output,hdecoder=self.decoder.forward(nextinp.to(device),hdecoder)\n","                outputs[t]=output\n","                t+=1\n","        else:            \n","            for i in range(1,HINDILEN):\n","                nextinp=target[i,:].unsqueeze(0)\n","                output,hdecoder=self.decoder.forward(nextinp.to(device),hdecoder)\n","                outputs[t]=output\n","                t+=1\n","        return outputs\n","        \n","    \n","encoder=EncoderRNN(len(englishvocab),256,256,2).to(device)\n","decoder=DecoderRNN(len(hindivocab),256,256,2,len(hindivocab)).to(device)\n","seq2seq=Seq2Seq(encoder,decoder)\n","def train():\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","    loss=0\n","    count=0\n","    numbatches=englishwords.shape[0]//BATCH_SIZE\n","    for ep in range(5):\n","        trainloss=0\n","        for i in range(numbatches):\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            temp=englishwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temph=hindiwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","            temp=temp.t()\n","            temph=temph.t()\n","            output=seq2seq.forward(temp,temph)\n","            output = output[:].reshape(-1, output.shape[2])\n","            tem = temph[:].reshape(-1)\n","            loss=criterion(output,tem)\n","            loss.backward()\n","            trainloss+=loss.item()/(BATCH_SIZE*HINDILEN)\n","            torch.nn.utils.clip_grad_norm_(decoder.parameters(),max_norm = 1)\n","            torch.nn.utils.clip_grad_norm_(encoder.parameters(),max_norm = 1)\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","        print(trainloss)\n","    \n","\n","train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["18511\n"]}],"source":["numbatches=englishwords.shape[0]//BATCH_SIZE\n","correct=0\n","for i in range(numbatches):\n","    temp=englishwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","    temph=hindiwords[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n","    temp=temp.t()\n","    temph=temph.t()\n","    output=seq2seq.forward(temp,temph)\n","    output=nn.Softmax(dim=2)(output)\n","    output=torch.argmax(output,dim=2)\n","    temph=temph.t()\n","    output=output.t()\n","    for i in range(BATCH_SIZE):\n","        if(torch.equal(output[i],temph[i])):\n","            correct+=1\n","\n","print(correct)\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"embedding(): argument 'indices' (position 2) must be Tensor, not str","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n","\u001b[0;32m/tmp/ipykernel_27/2902298691.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[1;32m      2\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m      3\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m----> 4\u001b[0;31m \u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"g\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/948261357.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, hidden)\u001b[0m\n","\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m---> 10\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n","\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n","\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n","\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n","\u001b[1;32m    160\u001b[0m         return F.embedding(\n","\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m--> 162\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n","\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\n","\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not str"]}],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
